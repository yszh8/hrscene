<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="How Far Are VLMs from Effective High-Resolution Image Understanding?">
  <meta name="keywords" content="HRScene">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title> HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
/Users/panlu/Library/Mobile Documents/com~apple~CloudDocs/ImageMath/visual-mathqa-server/data_final/images
    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link rel="icon" href="./static/images/icons/hrscene.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">

  <!-- external libs -->
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>

  <!-- dynamic html -->
  <script type="text/javascript" src="./static/js/sort-table.js" defer></script>
  <script src="./static/js/explorer-index.js"></script>
  <script src="./static/js/question_card.js"></script>
  <script src="./static/js/leaderboard_testmini.js"></script>  

  <!-- data variables -->
  <script src="./data/leaderboard/results.js" defer></script>
  <script src="./data/explorer/data.js" defer></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->
      <!-- @PAN TODO: consider adding links? -->
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://research.google/blog/chain-of-agents-large-language-models-collaborating-on-long-context-tasks/">
            <b>Chain-of-Agents (CoA) </b>
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="static/images/icons/hrscene.png" style="width:1em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista" style="vertical-align: middle">HRScene</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            How Far Are VLMs from Effective High-Resolution Image Understanding?
            <!-- <br> -->
            <!-- with GPT-4V, Bard, and Other Large Multimodal Models -->
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup><a href="https://yuszh.com/">Yusen Zhang</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://www.linkedin.com/in/wenliang-zheng-b8a390291">Wenliang Zheng</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://in.linkedin.com/in/aashrith-madasu-170771217">Aashrith Madasu</a>,
            </span>
            <span class="author-block">
              <sup>2</sup><a href="https://www.amazon.science/author/peng-shi">Peng Shi</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://ryokamoi.github.io/">Ryo Kamoi</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://hzhou3.github.io/">Hao Zhou</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://www.linkedin.com/in/zhuoyang-zou-44b2b3238">Zhuoyang Zou</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://shuzhao.me/">Shu Zhao</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://sarathismg.github.io/">Sarkar Snigdha Sarathi Das</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://vipulgupta1011.github.io/">Vipul Gupta</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://openreview.net/profile?id=~Xiaoxin_Lu1">Xiaoxin Lu</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://zn1010.github.io/">Nan Zhang</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://windchimeran.github.io/">Ranran Haoran Zhang</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://www.linkedin.com/in/avitej-iyer">Avitej Iyer</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://renzelou.github.io/">Renze Lou</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://www.wenpengyin.org/">Wenpeng Yin</a>,
            </span>
            <span class="author-block">
              <sup>1</sup><a href="https://ryanzhumich.github.io/">Rui Zhang</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <br>
            <span class="author-block">
              <sup>1</sup>
              <img class="aff-icon" src="static/images/icons/psu.jpg">Pennsylvania State University
            </span>
            <span class="author-block">
              <sup>2</sup>
              <img class="aff-icon" src="static/images/icons/aws.png">Amazon Web Services
            </span>
            <br>
            <!-- <span class="author-block"><sup style="color:#ffac33">3</sup>Microsoft Research</span><br> -->
            <!-- <span class="paper-block"><b style="color:#f41c1c">ICLR 2024 Oral</b> (85 in 7304, 1.2%)</span> -->
          </div>
        
          <!-- <section> -->
            <!-- <div class="section" id="org-banners" style="display:fle">
              <a href="https://www.ucla.edu/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/ucla.png" style="height:3em">
              </a>
              <a href="https://www.washington.edu/" target="blank" class="ext-link">
                  <img class="center-block org-banner" src="static/images/uw.png" style="height:3em">
              </a>
              <a href="https://www.microsoft.com/en-us/research/" target="_blank" rel="external">
                  <img class="center-block org-banner" src="static/images/microsoft.png" style="height:3em">
              </a>
            </div> -->
          <!-- </section> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->

              <!-- <span class="link-block">
                <a href="https://github.com/yszh8/hrscene/blob/e86ee902c8edd7022d6daa89fdb30367b33601d0/static/pdfs/HRScene.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              <!-- <span class="link-block">
                <a href="https://yszh8.github.io/hrbench/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/WenliangZhoushan/HRScene"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Wenliang04/HRScene"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🤗</p>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <!-- Visualization Link. -->
              <span class="link-block">
                <a href="#explorer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🔍</p>
                  </span>
                  <span>Data Explorer</span>
                </a>
              </span>

              <!-- Leaderboard Link. -->
              <span class="link-block">
                <a href="#leaderboard"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🏆</p>
                  </span>
                  <span>Leaderboard</span>
                </a>
              </span>

              <!-- Twitter Link. -->
              <!-- <span class="link-block">
                <a href="https://yszh8.github.io/hrbench/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <p style="font-size:18px">🌐</p>
                  </span>
                  <span>Twitter</span>
                </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/tease_scores_gpt4v.png" alt="geometric reasoning" width="99%"/>
      <p> Accuracy scores of one leading LLM (i.e., PoT GPT-4), four primary LMMs, random chance, and human performance our proposed 
      <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
      <span class="mathvista">HRScene</span>
      across mathematical reasoning and visual context types. PoT refers to program-of-thought prompting, and PoT GPT-4 is a textual LLM augmented with the caption and OCR text. GPT-4V is manually evaluated via the playground chatbot.
      </p>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">

              <img src="static/images/figure1c.png" alt="geometric reasoning" width="50%" />

              <p> 
                  Comparison between the benchmarks that the models are evaluated and 
                  <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>. 
                  The y-axis is the square root of the total pixel. The boxes/icons indicate the image resolution they contain/support. 
                  The black lines inside each box show the average resolutions. For most of the vision-based benchmarks 
                  that VLMs are evaluated on, the average resolution is typically below 1k, making them unsuitable for HRI evaluation. 
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/radar_chart_all_models.png" alt="geometric reasoning" width="40%"/>

              <p>
                Performance of some VLMs on our test set. We identify 8 categories of HRI tasks:  Daily pictures, Urban planning, 
                Paper scanned images, Artwork, Multi-subimages, Remote sensing, Medical Diagnosing, and Research understanding. 
                Experiments on real-world tasks demonstrate that current VLMs perform modestly, with an average accuracy of around 50%, 
                highlighting substantial challenges of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>. 
                Besides, we also provide the human performance of all real-world datasets by engaging graduate-level annotators to annotate 750 image-question pairs.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>


<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <!-- <p>
          <b>Large Language Models (LLMs)</b> and <b>Large Multimodal Models (LMMs)</b> exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied.
          </p>
          <p>
            To bridge this gap, we present <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span>, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of <b>6,141 examples</b>, derived from <b>28 existing multimodal datasets</b> involving mathematics and <b>3 newly created datasets</b> (i.e., <b>IQTest, FunctionQA, and PaperQA</b>). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. 
          </p>
          <p>
            With <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span>, we have conducted <b>a comprehensive, quantitative evaluation of 12 prominent foundation models</b>. The best-performing <b>GPT-4V</b> model achieves an overall accuracy of <b>49.9%</b>, substantially outperforming Bard, the second-best performer, by <b>15.1%</b>. Our in-depth analysis reveals that the superiority of <b>GPT-4V</b> is mainly attributed to its enhanced visual perception and mathematical reasoning. However, <b>GPT-4V</b> still falls short of human performance by <b>10.4%</b>, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span> will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of <b>self-verification</b>, the use of <b>self-consistency</b>, and the  <b>goal-directed multi-turn human-AI dialogues</b>, highlighting the promising potential of GPT-4V for future research. 
          </p> -->

          <p>
            High-resolution image (HRI) understanding aims to process
            images with a large number of pixels such as pathological images and agricultural aerial images, both of which
            can exceed 1 million pixels. Vision Large Language Models (VLMs) typically handle higher-resolution images through
            dynamic patching. However, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding,
            leaving this domain underexplored. To address this gap, we introduce 
            <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>, 
            a novel unified benchmark for HRI understanding with rich scenes. 
            <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>
            incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with
            resolutions ranging from 1,024 × 1,024 to 35,503 × 26,627. 
            <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span> 
            is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic and radiology images to street views, 
            long-range pictures, and telescope images. It includes high-resolution images of real-world objects, scanned documents, 
            and composite multi-image. The two diagnostic evaluation datasets
            are synthesized by combining the target image with the gold answer and similar distracting images in different orders.
            These datasets assess how well models utilize HRI by comparing performance across different image regions. 
            We conduct extensive experiments involving 27 VLMs, including Gemini 2.0 Pro and GPT-4o. 
            Experiments on <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>
            show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps
            in HRI understanding. Results on our synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions
            compared to low-resolution images, with a gap exceeding 20%. Our code and data will be publicly available.
          </p>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <div class="container">
    
    <div class="columns is-centered">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3" id="leaderboard">Leaderboard on HRScene (testmini)</h2>
        <div class="content">
          <p class="mt-3">Accuracy scores on the <b>testmini</b> subset (1,000 examples) of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span>.
          </p>

          <div id="testmini_leaderboard"></div>

        </div>

        <h2 class="title is-3" id="leaderboard_test">Leaderboard on HRScene (test)</h2>
        <div class="content">
          <p class="mt-3">Accuracy scores on the <b>test</b> subset (5,323 examples with <b>private</b> ground truth) of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span>.
          </p>

          <div id="test_leaderboard"></div>

          <!-- <b>Human Performance*:</b> Average human performance from AMT annotators who have high school diplomas or above.
          <br>
          <b>Method types:</b> <b>MoE 🤖:</b> Mixture of Experts, <b>LMM 🖼️:</b> Large Multimodal Model, <b>Tool 🛠️:</b> Tool-augmented Large Language Model.
          <br>
          <b>Task types:</b> <b>FQA:</b> figure QA,
          <b>GPS:</b> geometry problem solving,
          <b>MWP:</b> math word problem,
          <b>TQA:</b> textbook QA,
          <b>VQA:</b> visual QA.
          <br>
          <b>Math reasoning types:</b> 
          <b>ALG:</b> algebraic,
          <b>ARI:</b> arithmetic,
          <b>GEO:</b> geometry,
          <b>LOG:</b> logical ,
          <b>NUM:</b> numeric,
          <b>SCI:</b> scientific,
          <b>STA:</b> statistical.
          <br>
          <br> -->
          <div>
          <!-- <p>
            🚨 To submit your results to the leaderboard, please send to <a href="mailto:lupantech@gmail.com">this email</a> with your result json files.
          </p>
          <p>
            🚨 For more submission details, please refer to <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-leaderboard-">this link</a> and <a href="https://github.com/lupantech/MathVista?tab=readme-ov-file#-evaluations-on-mathvista">this link</a>.
          </p> -->
          <p>
            🚨 Open source models are marked in <b style="color: darkgreen;">Green</b> and Closed source models are marked in <b style="color: darkred;">Red</b> 
          </p>

          </div>
        </div>

      </div>
    </div>

  </div>
</section>

<!-- DATASET SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <!-- <h1 class="title is-1 mathvista"><img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>MathVista Dataset</h1> -->
  <h1 class="title is-1 mathvista">
    <img src="static/images/icons/hrscene.png" style="width:1em;vertical-align: middle" alt="Logo"/>
    <span class="mathvista" style="vertical-align: middle">HRScene Dataset</span>
  </h1>
  </div>
</section>

<!-- <section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p> -->
            
<section class="section">
  <div class="container">
    <div class="columns is-centered has-text-centered">
      <!-- <div class="column is-full-width has-text-centered"> -->
        <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span> is collected from 25 existing data resources and 8 of them are re-annotated by 10 graduate-level annotators, 
            with diverse view scales, ranging from microscope to radiology, street views, long-range, and telescope images. 
            It contains high-resolution images of real objects, electronic documents, and composite multi-subimages. 
            Besides, six datasets require domain-expert knowledge, while the remaining 19 belong to general domains. 
            The diagnostic dataset is synthesized by combining the target image with the gold answer and visually similar distractors 
            arranged in different orders to assess HRI utilization. Overall, <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span> comprises 7,081 images, with 2,008 of them being re-annotated.
            <!-- a compilation of data 1) carefully examined and filtered from 28 existing VQA and MathQA datasets and 2) manually collected by us. In total, 6,141 examples were collected from 31 different datasets. -->
          </p>

          <div id="results-carousel" class="carousel results-carousel">

            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/fig2.png" alt="algebraic reasoning" width="50%"/>
                <p> Distribution of resolution of each dataset. X-axis is the resolution and $n$k indicates the resolution is at least $n^2*10^6$ pixels.</p>
              </div>
            </div>

            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/fig3.png" alt="algebraic reasoning" width="70%" style="margin-top: 150px"/>
                <p> Some examples of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>. Blue ones are diagnostic datasets and purple ones are real-world datasets.</p>
              </div>
            </div>

            <div class="box m-5">
              <div class="content has-text-centered">
                <img src="static/images/tab7.png" alt="algebraic reasoning" width="80%"/>
                <p> Overview of 25 real-world datasets and their statistics. * indicates that the dataset is reannotated.</p>
              </div>
            </div>

          </div>

          <!-- <div class="content has-text-centered">
            <img src="static/images/source_dataset.png" alt="geometric reasoning" width="60%"/>
            <p> Summary of the 31 different source datasets in <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">HRScene</span>.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="static/images/our_new_3_datasets.png" alt="geometric reasoning" width="70%"/>
            <p> Examples of our newly annotated datasets: IQTest, FunctionQA, and PaperQA.</p>
          </div> -->

          <p>
            <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>  
            consists of 7,073 samples, divided into three splits:
            <!-- @PAN TODO: add download links -->
            <ul>
              <li><b>val</b> contains 750 samples. These samples are identical to human-annotated ones, designed for fine-grained validation of the users' VLM settings.</li>
              <li><b>testmini</b> comprises 1,000 samples, picked from each <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/><span class="mathvista">HRScene</span>  
                real-world dataset, intended for rapid model development evaluation or for those with limited computing resources.</li>
              <li><b>test</b> features the remaining 5,323 samples for standard evaluation. Notably, the answer labels for <b>test</b> will 
                not be publicly released to facilitate fair evaluation. Instead, we maintain an online evaluation platform for user submissions.</li>
            </ul>
            You can download the dataset on <a href="https://huggingface.co/datasets/Wenliang04/HRScene" target="_blank">Hugging Face Dataset</a>.
          </p>

        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column" style="margin-right: -20rem;">
        <div class="content has-text-centered">
          <img src="static/images/tab2.png" alt="data-overview" style="max-width: 60%; margin-top: 200px;"/>
          <p> 
            Key statistics of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
            <span class="mathvista">HRScene</span>.<br/>
          </p> 
        </div>
      </div>
      <div class="column">
        <div class="content has-text-centered">
          <img src="static/images/fig1a.png" alt="data-composition" style="max-width: 56%;"/>
          <p>
              Source dataset distribution of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
              <span class="mathvista">HRScene</span>.<br/>
              <!-- <b>FQA</b>: figure question answering,
              <b>GPS</b>: geometry problem solving,<br/>
              <b>MWP</b>: math word problem,
              <b>TQA</b>: textbook question answering,<br/>
              <b>VQA</b>: visual question answering. -->
          </p>
        </div>
      </div>
    </div>

    
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Examples</h2>
        <p>One example for each <b>category</b> in <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
          <span class="mathvista">HRScene</span></p>

        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Daily</b></p>
              <img src="static/images/examples/daily_hrbench.png" alt="arithmetic reasoning" width="25%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Research</b></p>
              <img src="static/images/examples/research_galaxy.png" alt="algebraic reasoning" width="25%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Medical</b></p>
              <img src="static/images/examples/medical_vqarad.png" alt="geometric reasoning" width="25%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Sub Image</b></p>
              <img src="static/images/examples/subimg_visdiff.png" alt="logical reasoning" width="30%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Remote Sensing</b></p>
              <img src="static/images/examples/remote_sen.png" alt="numeric reasoning" width="25%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Art</b></p>
              <img src="static/images/examples/art_artbench.png" alt="statistical reasoning" width="20%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Paper</b></p>
              <img src="static/images/examples/paper_docstruct.png" alt="scientific reasoning" width="25%"/>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <p><b>Urban</b></p>
              <img src="static/images/examples/urban.png" alt="scientific reasoning" width="25%"/>
            </div>
          </div>

          
        </div>

        <!-- <br><br>
        <p>One example for each <b>visual context</b> type required in <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
          <span class="mathvista">HRScene</span></p>

        <div id="results-carousel" class="carousel results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/1_Geometry_Diagram.png" alt="arithmetic reasoning" width="80%"/>
              <p>Geometry Diagram</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/2_Synthetic_Scene.png" alt="arithmetic reasoning" width="80%"/>
              <p>Synthetic Scene</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/3_Bar_Chart.png" alt="arithmetic reasoning" width="65%"/>
              <p>Bar Chart</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/4_Natural_Image.png" alt="arithmetic reasoning" width="80%"/>
              <p>Natural Image</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/5_Scientific_Figure.png" alt="arithmetic reasoning" width="80%"/>
              <p>Scientific Figure</p>
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/6_Table.png" alt="arithmetic reasoning" width="80%"/>
              <p>Table</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/7_Function_Plot.png" alt="arithmetic reasoning" width="80%"/>
              <p>Function Plot</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/8_Abstract_Scene.png" alt="arithmetic reasoning" width="80%"/>
              <p>Abstract Scene</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/9_Puzzle_Test.png" alt="arithmetic reasoning" width="80%"/>
              <p>Puzzle Test</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/10_Scatter_Plot.png" alt="arithmetic reasoning" width="80%"/>
              <p>Scatter Plot</p>
            </div>
          </div>
          
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/11_Line_Plot.png" alt="arithmetic reasoning" width="80%"/>
              <p>Line Plot</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/12_Pie_Chart.png" alt="arithmetic reasoning" width="75%"/>
              <p>Pie Chart</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/13_Document_Image.png" alt="arithmetic reasoning" width="80%"/>
              <p>Document Image</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/14_Medical_Image.png" alt="arithmetic reasoning" width="80%"/>
              <p>Medical Image</p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/contexts/15_Others.png" alt="arithmetic reasoning" width="45%"/>
              <p>Others</p>
            </div>
          </div> -->

        </div>

      </div>
    </div>




    <div class="columns is-centered m-6" style="display: none;">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Statistics</h2>
        <p>Notable statistics of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
          <span class="mathvista">HRScene</span></p>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/context-dist.png" alt="context" class="stats-image">
              <p>Distribution of visual context types within <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/category.png" alt="category" class="stats-image"/>
              <p>Category distribution of problems within <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/grade-levels.png" alt="grade" class="stats-image"/>
              <p>Distribution of questions across different grade levels within <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/qs-len.png" alt="qs-len" class="stats-image"/>
              <p>Distribution of the number of words per question in <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span>. 
                <!-- Questions with a length greater than 60 are categorized as 61 for visualization simplicity -->
              </p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/reasoning.png" alt="reasoning" class="stats-image"/>
              <p>Portion of each mathematical reasoning type involved in the problems of <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/reasoning-count.png" alt="reasoning-count" class="stats-image"/>
              <p>Distribution of the number of mathematical reasoning types within <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span></p>
            </div>
          </div>
          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/statistics/task-type.png" alt="task-type" class="stats-image"/>
              <p>Task type distribution of problems within <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span></p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- <div class="columns is-centered m-6">
      <div class="column is-max-desktop has-text-centered">
        <h2 class="title is-3" id="visualization">Visualization</h2>
        <iframe src="visualizer/explore.html" style="width: 100%;min-height: 100vh; border-radius: 20px;"></iframe>
      </div>
    </div> -->
    
  </div>
</section>

<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Experiment Results</h1>
  </div>
</section>

<section class="section">
  <div class="container">

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Results on 25 Real-World Datasets</h2>
        
        <div id="results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/tab3.png" alt="grade-lv" width="50%"/>
              <p>
                Overall results of all models on real-world datasets of 
                <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
                <span class="mathvista">HRScene</span>. <br><br>

                The models are clustered according to the parameter sizes. 
                <b>Bold</b> indicates global best performance, while <u>underline</u> represents the best of the group. 
                Avg is the mean value of the column/row.  

                Results show that due to the native resolution support of Qwen, it obtains SOTA even general capability might not be the best. 
                This result highlights <b>the importance of the HRI processing capability of native resolution to obtain high performance.</b> 
                However, <b>the average performance across all categories is only 48.54%, showing the large gap between VLMs and efficient HRI processing.</b>
              </p>
            </div>
          </div>

          <h2 class="title is-3">Results on 2 Diagnosis Datasets</h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <b>White Background</b><br>
              <img src="static/images/tab4.png" alt="grade-lv" width="70%"/>
              <p>
                Table shows the statistics of the <b>WhiteBackground diagnosis</b>. <br><br>
                We report the average performance of the samples (Perf &uarr;), the performance drop with image size increasing from 1x1 (Size &uarr;), 
                and the region expectation gap (Region &darr;), which is the difference between the highest performance region and the mean 
                performance of every region. We call this <b><i>Regional Divergence</i></b>. As shown in Table, most of the models cannot maintain consistent 
                performance with increasing image size. Furthermore, models exhibit significant Region Divergence, usually amplified with increasing image size.
              </p>
            </div>
          </div>

          <div class="box m-5">
            <div class="content has-text-centered">
              <b>Complex Grid</b><br>
              <img src="static/images/fig4.png" alt="grade-lv" width="90%"/>
              <p>
                Surprisingly, We observe a phenomenon that is similar to lost-in-the-middle. 
                Figure shows the performance change of the models with increasing Manhattan distance from row 1, column 1 to the needle image. 
                Differently, we observe the performance forms a U-shape based on the Manhattan distance from the left upper corner rather than 
                the linear depth of the needle in traditional NIAH. We call this Lost-in-the-middle Manhattan.
              </p>
            </div>
          </div>

        </div>
      </div>
    </div>

    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Visualization Examples</h2>
        <div id="results-carousel">

          <div class="box m-5">
            <div class="content has-text-centered">
              <img src="static/images/fig6.png" alt="" width="80%" style="margin-top: 20px;"/>
              <br>
              <br>
             <p>Detailed performance of some models on two diagnose datasets.</p>
            </div>
          </div>
          
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
        We also provide a convenient tool to diagnose your own model with 5 lines of code! 
        Here is the&#20;<a href="https://huggingface.co/datasets/Wenliang04/HRScene" target="_blank">Hugging Face Dataset</a>
    </div>

    <div class="container is-full has-text-centered content m-6" id="result-table">
      <h2 class="title is-3" id="explorer">Explorer</h2>
      <p>Explore the outputs of each model on <img src="static/images/icons/hrscene.png" style="width:1.0em;vertical-align: middle" alt="Logo"/>
        <span class="mathvista">HRScene</span></p>
      <div class="level has-text-centered" style="position: sticky; top: 0; z-index: 20;">
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <button class="button" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);" id="refresh-qids">
            <span class="icon is-large">
              <i class="fa fa-redo fa-lg" aria-hidden="true"></i>
            </span>
            <p class="title is-4 m-0">Refresh Question</p>
          </button>
        </div>
        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <div class="dropdown" style="width: 100%;">
            <div class="dropdown-trigger has-text-justified" style="width: 100%; ">
              <button class="button" aria-haspopup="true" aria-controls="dropdown-menu" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);">
                <p class="title m-0 is-4 dropdown-display">Claude-Haiku</p>
                <span class="icon is-large" style="position: absolute; right:0;">
                  <i class="fas fa-angle-down fa-lg" aria-hidden="true"></i>
                </span>
              </button>
            </div>
            <div class="dropdown-menu" id="dropdown-menu" role="menu" style="width:100%;">
              <div class="dropdown-content">
              
              </div>
            </div>
          </div>
        </div>

        <div class="level-item box m-3" style="width: 30%; background: rgba(250, 250, 250, 1);">
          <div class="dropdown" style="width: 100%;">
            <div class="dropdown-trigger has-text-justified" style="width: 100%;">
              <button class="button" aria-haspopup="true" aria-controls="dropdown-menu" style="width: 100%; border: none; background: rgba(250, 250, 250, 1);">
                <p class="title m-0 is-4 dropdown-display">Claude-Sonnet</p>
                <span class="icon is-large" style="position: absolute; right:0;">
                  <i class="fas fa-angle-down fa-lg" aria-hidden="true"></i>
                </span>
              </button>
            </div>
            <div class="dropdown-menu" id="dropdown-menu" role="menu" style="width:100%;">
              <div class="dropdown-content">
               
              </div>
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


<!-- @PAN TODO: bibtex -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <pre><code>
      @article{zhang2025hrscene,
        title={HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?},
        author={Zhang, Yusen and Zheng, Wenliang and Madasu, Aashrith and Shi, Peng and Kamoi, Ryo and Zhou, Hao and Zou, Zhuoyang and Zhao, Shu and Das, Sarkar Snigdha Sarathi and Gupta, Vipul and Lu, Xiaoxin and Zhang, Nan and Zhang, Ranran Haoran and Iyer, Avitej and Lou, Renze and Yin, Wenpeng and Zhang, Rui},
        journal={arXiv preprint},
        year={2025}
      }
    </code></pre>
  </div>
</section>

<!-- <section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/icons/psu.jpg">
    </a>
    <a href="" target="blank" class="ext-link">
        <img class="center-block org-banner" src="static/images/icons/aws.png">
    </a>
  </div>
</section> -->


<footer class="footer">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center;">
            Developed by <a href="https://in.linkedin.com/in/aashrith-madasu-170771217">Aashrith Madasu</a>, adapted from 
            <a href="https://mathvista.github.io/">MathVista</a>.
            <br>
            Feel free to use the <a href="https://github.com/yszh8/hrscene">code</a> with a link back to this page in the footer.
            <br>
            Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
          </p>
        </div>
      </div>
    </div>
</footer>

</body>
</html>
